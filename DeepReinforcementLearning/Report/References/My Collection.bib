@book{Sutton2018,
  abstract = {This chapter provides a concise introduction to Reinforcement Learning (RL) from a machine learning perspective. It provides the required background to understand the chapters related to RL in this book. It makes no assumption on previous knowledge in this research area and includes short descriptions of some of the latest trends, which are normally excluded from other introductions or overviews on RL. The chapter provides more emphasis on the general conceptual framework and ideas of RL rather than on presenting a rigorous mathematical discussion that may require a great deal of effort by the reader. The first section provides a general introduction to the area. The following section describes the most common solution techniques. In the third section, some of the most recent techniques proposed to deal with large search spaces are described. Finally, the last section provides some final remarks and current research challenges in RL. {\textcopyright} 2012, IGI Global.},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/book - Reinforcement Learning An Introduction, 2nd Edition by Richard S. Sutton, Andrew G Barto (z-lib.org).pdf:pdf},
  isbn = {9780262039246},
  pages = {481},
  title = {{Reinforcement Leaning}},
  year = {2018}
}
@article{Sutton2017,
  abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  doi = {10.1016/s1364-6613(99)01331-5},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/bookdraft2017nov5.pdf:pdf},
  issn = {13646613},
  journal = {Trends in Cognitive Sciences},
  number = {9},
  pages = {360},
  title = {{Reinforcement Learning: An Introduction, draft 2017 nov.5}},
  volume = {3},
  year = {2017}
}
@misc{Lanham2020,
  abstract = {The AI revolution is here and it is embracing games. Game developers are being challenged to enlist cutting edge AI as part of their games. In this book, you will look at the journey of building capable AI using reinforcement learning algorithms and techniques. You will learn to solve complex tasks and build next-generation games using a ...},
  author = {Lanham, Micheal},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/Hands-On Reinforcement Learning for Games Implementing self-learning agents in games using artificial intelligence techniques (Micheal Lanham) (z-lib.org).pdf:pdf},
  isbn = {9781839214936},
  title = {{Hands-on reinforcement learning for games : implementing self-learning agents in games using artificial intelligence techniques}},
  year = {2020}
}
@article{Yu2019,
  abstract = {The main objective of reinforcement learning (RL) is to enable an agent to act optimally to maximize the cumulative long-term reward. Q-learning is a model free RL algorithm, which iteratively learns a long-term reward function "Q" given the current state and action. The Deep Q-Learning Network (DQN) facilitates the Q-learning by modeling the Q-function as a neural network. This project implements and experiments such DQN models on the OpenAI Gym's LunarLander-v2 environment, using a two-layer feed-forward network with a technique named "experience replay". Extensive experiments are done to determine the neural network size and tune various hyper-parameters including learning rate \alpha, reward discount factor \gamma and exploration-exploitation trade-off \epsilon. Major findings include 1) the Lunar Lander favors a large hidden layer but not a deeper network; 2) a near-one reward discount is necessary for the model to consider final successful landing. Finally, our best model can stably achieve 280+ mean reward for a trial of 100 landing episodes. The code can be found at https://github.com/XinliYu/RL-Projects.},
  author = {Yu, Xinli},
  file = {:C\:/Users/Daniel Kuknyo/Documents/ELTE/2/Artificial Intelligence in Processes and Automation/Project/Report/References/dqn_lunar_lander.pdf:pdf},
  number = {May},
  pages = {5},
  title = {{Deep Q-Learning on Lunar Lander Game}},
  url = {https://www.researchgate.net/publication/333145451_Deep_Q-Learning_on_Lunar_Lander_Game},
  year = {2019}
}
@article{xu2018deep,
  title={Deep reinforcement learning with sarsa and Q-learning: a hybrid approach},
  author={Xu, Zhi-xiong and Cao, Lei and Chen, Xi-liang and Li, Chen-xi and Zhang, Yong-liang and Lai, Jun},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={101},
  number={9},
  pages={2315--2322},
  year={2018},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}
@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{simonini2018improvements,
  title={Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets},
  author={Simonini, Thomas},
  year={2018},
  publisher={{\v{C}}ervenec}
}
@inproceedings{wang2016dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}