{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "casual-satin",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning homework 1\n",
    "### Daniel Kuknyo\n",
    "Consider a 2-armed bandit situation. You are in a hurry driving a car and want to go from Buda to Pest. You have to get there early since you have an important meeting. Unfortunately, there are only two bridges, and you have to take one to cross the river.\n",
    "\n",
    "Sometimes there is heavy traffic on the bridges, and it is possible that you jam on them, and miss your meeting. If the traffic is regular, then the crossing time is shorter. For both bridges, there is a different p probability of the jam. The reward (here, it will be a negative number) of the jam and the regular pass is different. The reward means the negative value of the time you need to cross the bridge. Note that you could also talk about costs that should be minimized. It would be more natural to use costs here, but in RL, most works talk about rewards.\n",
    "\n",
    "Optimize the exploration-exploitation strategy to minimize your time (maximize reward) during 1000 steps (episodes). Exploration: the number of random choices between the two bridges. Exploitation: the sudden or gradual switch to the bridge that you consider better.\n",
    "\n",
    "Youâ€™ll be given the jamming probability of the bridges and also the rewards. You can trivially compute the solution. Compute it. This way, you know what the best you could achieve is. However, your agent knows nothing and learns from experiences... Run a sufficient number of random tests to show the improving (average) performance throughout the 1000 learning steps.\n",
    "\n",
    "Create a graph about the rewards during the 1000 steps. In what circumstances (jamming probabilities) could this problem be hard or easy? Why?\n",
    "\n",
    "You have to use Python during the task and have to upload the solution in Jupyter notebook (.ipynb) format. If you are not familiar with it, here is a quick and easy tutorial: https://www.dataquest.io/blog/jupyter-notebook-tutorial/ (Links to an external site.)\n",
    "\n",
    "You'll find two sets of model values in the Homework_1_input_data.csv under your Neptun code. Solve the task for both set. If you have any questions feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "light-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordinary-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hyperparameters\n",
    "APjam = 0.18\n",
    "ARewardJam = 49\n",
    "ARewardNormal = 9\n",
    "\n",
    "BPjam = 0.29\n",
    "BRewardJam = 46\n",
    "BRewardNormal = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enabling-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eps_bandit:\n",
    "    '''\n",
    "    k-bandit problem\n",
    "    \n",
    "    Inputs\n",
    "    =====================================================\n",
    "    k: number of arms (int)\n",
    "    \n",
    "    eps: probability of random action 0 < eps < 1 (float)\n",
    "    \n",
    "    iters: number of steps (int)\n",
    "    \n",
    "    mu: set the average rewards for each of the k-arms.\n",
    "        Set to \"random\" for the rewards to be selected from\n",
    "        a normal distribution with mean = 0. \n",
    "        Set to \"sequence\" for the means to be ordered from \n",
    "        0 to k-1.\n",
    "        Pass a list or array of length = k for user-defined\n",
    "        values.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, k, eps, iters, mu='random'):\n",
    "        # Number of arms\n",
    "        self.k = k\n",
    "        # Search probability\n",
    "        self.eps = eps\n",
    "        # Number of iterations\n",
    "        self.iters = iters\n",
    "        # Step count\n",
    "        self.n = 0\n",
    "        # Step count for each arm\n",
    "        self.k_n = np.zeros(k)\n",
    "        # Total mean reward\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        # Mean reward for each arm\n",
    "        self.k_reward = np.zeros(k)\n",
    "        \n",
    "        if type(mu) == list or type(mu).__module__ == np.__name__:\n",
    "            # User-defined averages            \n",
    "            self.mu = np.array(mu)\n",
    "        elif mu == 'random':\n",
    "            # Draw means from probability distribution\n",
    "            self.mu = np.random.normal(0, 1, k)\n",
    "        elif mu == 'sequence':\n",
    "            # Increase the mean for each arm by one\n",
    "            self.mu = np.linspace(0, k-1, k)\n",
    "        \n",
    "    def pull(self):\n",
    "        # Generate random number\n",
    "        p = np.random.rand()\n",
    "        if self.eps == 0 and self.n == 0:\n",
    "            a = np.random.choice(self.k)\n",
    "        elif p < self.eps:\n",
    "            # Randomly select an action\n",
    "            a = np.random.choice(self.k)\n",
    "        else:\n",
    "            # Take greedy action\n",
    "            a = np.argmax(self.k_reward)\n",
    "            \n",
    "        reward = np.random.normal(self.mu[a], 1)\n",
    "        \n",
    "        # Update counts\n",
    "        self.n += 1\n",
    "        self.k_n[a] += 1\n",
    "        \n",
    "        # Update total\n",
    "        self.mean_reward = self.mean_reward + (reward - self.mean_reward) / self.n\n",
    "        \n",
    "        # Update results for a_k\n",
    "        self.k_reward[a] = self.k_reward[a] + (reward - self.k_reward[a]) / self.k_n[a]\n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.iters):\n",
    "            self.pull()\n",
    "            self.reward[i] = self.mean_reward\n",
    "            \n",
    "    def reset(self):\n",
    "        # Resets results while keeping settings\n",
    "        self.n = 0\n",
    "        self.k_n = np.zeros(k)\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        self.k_reward = np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "through-maine",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-261ea360994a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0miters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0meps_0_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mARewardJam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mARewardNormal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miters\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtile\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mtile\u001b[1;34m(A, reps)\u001b[0m\n\u001b[0;32m   1258\u001b[0m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m//=\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1260\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "APjam = 0.18\n",
    "ARewardJam = 49\n",
    "ARewardNormal = 9\n",
    "\n",
    "k = 2\n",
    "iters = 1000\n",
    "\n",
    "eps_0_rewards = np.\n",
    "\n",
    "episodes = 1000\n",
    "\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    # Initialize bandits\n",
    "    eps_0 = eps_bandit(k, APjam, iters)\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_0.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_0_rewards = eps_0_rewards + (eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(eps_0_rewards, label=\"$\\epsilon=0$ (greedy)\")\n",
    "plt.legend(bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Average $\\epsilon-greedy$ Rewards after \" + str(episodes) + \" Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
